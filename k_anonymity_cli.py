#!/usr/bin/env python
"""
K-Anonymity Evaluation CLI Tool

Computes k-anonymity metrics for multiple datasets using synthcity's kAnonymization.
Supports population, reference, training, and synthetic datasets.

Note: Synthcity uses a clustering-based approach to compute k-anonymity, which may differ
from traditional equivalence class-based k-anonymity.
"""

import json
from pathlib import Path
from typing import List, Optional, Dict
from datetime import datetime

import pandas as pd
import typer
from rich.console import Console
from rich.table import Table
from rich.panel import Panel
from sklearn.preprocessing import LabelEncoder
from sdv.metadata import SingleTableMetadata

from synthcity.plugins.core.dataloader import GenericDataLoader
from synthcity.metrics.eval_privacy import kAnonymization
from sdpype.metadata import load_csv_with_metadata

console = Console()
app = typer.Typer(add_completion=False)


def interpret_k_value(k: int) -> tuple[str, str]:
    """
    Interpret k-anonymity value and return (interpretation, color).

    Returns:
        tuple of (interpretation_text, rich_color)
    """
    if k >= 10:
        return "Excellent", "green"
    elif k >= 5:
        return "Good", "yellow"
    elif k >= 3:
        return "Moderate", "orange"
    else:
        return "Poor", "red"


def interpret_ratio(ratio: float) -> str:
    """Interpret k-ratio value."""
    if ratio > 1.5:
        return "Much better privacy"
    elif ratio > 1.1:
        return "Better privacy"
    elif ratio > 0.9:
        return "Similar privacy"
    elif ratio > 0.7:
        return "Worse privacy"
    else:
        return "Much worse privacy"


@app.command()
def main(
    population: Optional[Path] = typer.Option(None, "--population", help="Path to population dataset CSV."),
    reference: Optional[Path] = typer.Option(None, "--reference", help="Path to reference dataset CSV."),
    training: Optional[Path] = typer.Option(None, "--training", help="Path to training dataset CSV."),
    synthetic: Optional[Path] = typer.Option(None, "--synthetic", help="Path to synthetic dataset CSV."),
    metadata: Path = typer.Option(
        ...,
        "--metadata",
        help="Path to SDV metadata.json file (required for proper type enforcement).",
        exists=True,
        file_okay=True,
        dir_okay=False,
        readable=True
    ),
    qi_cols: str = typer.Option(
        ...,
        "--qi-cols",
        help="Comma-separated quasi-identifier columns.",
    ),
    sep: str = typer.Option(",", "--sep", help="CSV delimiter (only used as fallback if metadata loading fails)."),
    output: Optional[Path] = typer.Option(
        None,
        "--output",
        "-o",
        help="Save results to JSON file (optional)."
    ),
    experiment_name: str = typer.Option(
        "k_anonymity_evaluation",
        "--experiment-name",
        help="Experiment name for output metadata."
    ),
):
    """
    ðŸ”’ Compute k-anonymity metrics across multiple datasets.

    Supports population, reference, training, and synthetic datasets.
    Computes k-anonymity values for all provided datasets and k-ratios between relevant pairs.

    IMPORTANT: Requires metadata.json file for proper type enforcement and categorical detection.
    Data is loaded using metadata to ensure consistent dtypes (nullable Int64, categorical as str, etc.).
    Categorical columns are detected from metadata (sdtype='categorical'), not inferred from dtypes.
    """

    # Parse QI columns
    qi_list = [c.strip() for c in qi_cols.split(",") if c.strip()]

    # Collect provided datasets
    datasets_config = {
        "population": population,
        "reference": reference,
        "training": training,
        "synthetic": synthetic
    }

    provided_datasets = {name: path for name, path in datasets_config.items() if path is not None}

    # Validate at least 2 datasets provided
    if len(provided_datasets) < 2:
        console.print("âŒ [red]Error: At least 2 datasets must be provided[/red]\n", style="red")
        console.print("Available options:")
        console.print("  --population <path>")
        console.print("  --reference <path>")
        console.print("  --training <path>")
        console.print("  --synthetic <path>")
        raise typer.Exit(code=1)

    console.print(f"\nðŸ” K-Anonymity Evaluation", style="bold cyan")
    console.print(f"Datasets provided: {', '.join(provided_datasets.keys())}")
    console.print(f"QI columns: {qi_list}")
    console.print(f"Metadata: {metadata}\n")

    # Load metadata (single source of truth for column types)
    try:
        metadata_obj = SingleTableMetadata.load_from_json(str(metadata))
        console.print(f"âœ… Loaded metadata with {len(metadata_obj.columns)} columns\n")
    except Exception as e:
        console.print(f"âŒ Failed to load metadata: {e}", style="red")
        raise typer.Exit(code=1)

    # Load all datasets using metadata for type enforcement
    datasets = {}
    with console.status("[bold blue]Loading datasets with metadata-enforced dtypes..."):
        for name, path in provided_datasets.items():
            if not path.exists():
                console.print(f"âŒ File not found: {path}", style="red")
                raise typer.Exit(code=1)
            try:
                datasets[name] = load_csv_with_metadata(path, metadata)
                console.print(f"âœ… Loaded {name}: {datasets[name].shape[0]:,} rows Ã— {datasets[name].shape[1]} columns")
            except Exception as e:
                console.print(f"âŒ Failed to load {name}: {e}", style="red")
                raise typer.Exit(code=1)

    console.print()

    # Validate QI columns exist in all datasets
    for name, df in datasets.items():
        missing_cols = [c for c in qi_list if c not in df.columns]
        if missing_cols:
            console.print(f"âŒ QI columns missing from {name} dataset: {missing_cols}", style="red")
            raise typer.Exit(code=1)

    # Filter to only QI columns
    datasets_qi = {name: df[qi_list].copy() for name, df in datasets.items()}

    # Apply label encoding for categorical columns using metadata as single source of truth
    categorical_cols = []
    label_encoders = {}

    console.print("ðŸ”„ Detecting categorical columns from metadata...\n")

    for col in qi_list:
        # Use metadata as single source of truth for categorical detection
        if col not in metadata_obj.columns:
            console.print(f"âš ï¸  Warning: QI column '{col}' not found in metadata, skipping encoding", style="yellow")
            continue

        col_meta = metadata_obj.columns[col]
        is_categorical = col_meta.get("sdtype") == "categorical"

        if is_categorical:
            categorical_cols.append(col)
            console.print(f"ðŸ”„ Encoding categorical column: {col} (sdtype=categorical)")

            # Fit encoder on combined data from all datasets
            le = LabelEncoder()
            combined_values = pd.concat([df[col] for df in datasets_qi.values()]).astype(str)
            le.fit(combined_values)

            # Transform all datasets
            for name in datasets_qi.keys():
                datasets_qi[name][col] = le.transform(datasets_qi[name][col].astype(str))

            label_encoders[col] = le
            console.print(f"  â†’ Encoded {len(le.classes_)} categories: {list(le.classes_)}\n")

    if categorical_cols:
        console.print(f"âœ… Encoded {len(categorical_cols)} categorical column(s) from metadata: {categorical_cols}\n")
    else:
        console.print(f"âœ… No categorical QI columns found in metadata\n")

    # Compute k-anonymity for each dataset using synthcity
    console.print("ðŸ”’ Computing k-anonymity using synthcity kAnonymization...\n")

    k_values = {}
    k_anon_metric = kAnonymization()

    try:
        for name, df_qi in datasets_qi.items():
            with console.status(f"[bold blue]Computing k-anonymity for {name}..."):
                loader = GenericDataLoader(df_qi)
                # Use evaluate_data() for single dataset
                k = k_anon_metric.evaluate_data(loader)
                k_values[name] = int(k)
                console.print(f"âœ… {name}: k = {k}")

        console.print(f"\nâœ… All k-anonymity computations complete\n", style="green")

    except Exception as e:
        console.print(f"\nâŒ [red bold]Error: Synthcity kAnonymization failed[/red bold]", style="red")
        console.print(f"[red]Error details: {e}[/red]\n")
        raise typer.Exit(code=1)

    # Compute k-ratios for relevant pairs
    k_ratios = {}

    ratio_pairs = [
        ("reference", "population", "Reference / Population"),
        ("synthetic", "population", "Synthetic / Population"),
        ("synthetic", "reference", "Synthetic / Reference"),
        ("synthetic", "training", "Synthetic / Training"),
    ]

    for numerator, denominator, label in ratio_pairs:
        if numerator in k_values and denominator in k_values:
            ratio = k_values[numerator] / k_values[denominator] if k_values[denominator] > 0 else float("inf")
            k_ratios[label] = ratio

    # Display K-Anonymity Values Table
    k_values_table = Table(
        title="ðŸ”’ K-Anonymity Values",
        show_header=True,
        header_style="bold blue"
    )
    k_values_table.add_column("Dataset", style="cyan", no_wrap=True)
    k_values_table.add_column("k-anonymity", justify="right", style="white")
    k_values_table.add_column("Interpretation", style="yellow")

    for name in ["population", "reference", "training", "synthetic"]:
        if name in k_values:
            k = k_values[name]
            interp, color = interpret_k_value(k)
            k_values_table.add_row(
                name.capitalize(),
                f"[{color}]{k}[/{color}]",
                interp
            )

    console.print(k_values_table)
    console.print()

    # Display K-Ratios Table
    if k_ratios:
        k_ratios_table = Table(
            title="ðŸ“Š K-Anonymity Ratios",
            show_header=True,
            header_style="bold blue"
        )
        k_ratios_table.add_column("Comparison", style="cyan")
        k_ratios_table.add_column("Ratio", justify="right", style="white")
        k_ratios_table.add_column("Interpretation", style="yellow")

        for label, ratio in k_ratios.items():
            ratio_interp = interpret_ratio(ratio)
            # Color code ratios: green if > 1 (better), red if < 1 (worse)
            color = "green" if ratio > 1.0 else "red" if ratio < 0.9 else "yellow"
            k_ratios_table.add_row(
                label,
                f"[{color}]{ratio:.4f}[/{color}]",
                ratio_interp
            )

        console.print(k_ratios_table)
        console.print()

    # Interpretation guide
    guide_panel = Panel.fit(
        """ðŸ”’ K-Anonymity Evaluation Guide:

K-Anonymity Values:
â€¢ Higher k values indicate better privacy protection
â€¢ k â‰¥ 10: Excellent privacy protection
â€¢ k â‰¥ 5:  Good privacy protection
â€¢ k â‰¥ 3:  Moderate privacy protection
â€¢ k < 3:  Poor privacy protection

K-Ratios:
â€¢ Ratio > 1.0: Numerator dataset has better privacy than denominator
â€¢ Ratio < 1.0: Numerator dataset has worse privacy than denominator
â€¢ Synthetic / Population: Overall privacy improvement from synthesis
â€¢ Synthetic / Reference: Privacy protection on holdout data
â€¢ Synthetic / Training: Direct comparison with training data

Implementation Details:
â€¢ Metadata is the single source of truth for column types
â€¢ Categorical QI columns (sdtype='categorical') are label-encoded
â€¢ Data loaded with metadata-enforced dtypes (nullable Int64, etc.)
â€¢ Synthcity uses clustering-based approach, not traditional equivalence classes""",
        title="ðŸ“– Interpretation Guide",
        border_style="blue"
    )
    console.print(guide_panel)

    # Prepare results for JSON export
    results_dict = {
        "metadata": {
            "experiment_name": experiment_name,
            "evaluation_timestamp": datetime.now().isoformat(),
            "datasets": {
                name: {
                    "path": str(path),
                    "shape": list(datasets[name].shape)
                }
                for name, path in provided_datasets.items()
            },
            "qi_columns": qi_list,
            "categorical_columns": categorical_cols,
            "encoded_categories": {
                col: list(label_encoders[col].classes_)
                for col in categorical_cols
            } if categorical_cols else {},
            "evaluation_type": "k_anonymity",
            "method": "synthcity_kAnonymization"
        },
        "metrics": {
            "k_values": {
                name: {
                    "k": int(k),
                    "interpretation": interpret_k_value(k)[0]
                }
                for name, k in k_values.items()
            },
            "k_ratios": {
                label: {
                    "ratio": float(ratio),
                    "interpretation": interpret_ratio(ratio)
                }
                for label, ratio in k_ratios.items()
            }
        }
    }

    # Save to JSON if output path provided
    if output:
        output.parent.mkdir(parents=True, exist_ok=True)
        with open(output, 'w', encoding='utf-8') as f:
            json.dump(results_dict, f, indent=2, ensure_ascii=False)
        console.print(f"\nâœ… Results saved to: {output}", style="green")

    console.print("\nðŸŽ‰ K-anonymity evaluation complete!\n", style="bold green")


if __name__ == "__main__":
    app()
